{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QMCPy for Logistic Regression\n",
    "This notebook will give examples of how to use QMCPy for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qmcpy import *\n",
    "from numpy import *\n",
    "from qmcpy.integrand.LR import LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have some data and we want to use logistic regression for a dataset regarding admission to graduate school. The likelihood function for logistic regression, $f:\\mathbb{R}^{d+1} \\to [0,\\infty)$ takes the form\n",
    "\\begin{align*}\n",
    "f(\\boldsymbol{x}) & = \\prod_{i = 1}^m \n",
    "\\left(\\frac{\\exp\\left(x_0 + \\sum_{j = 1}^d x_j s_{ij} \\right)}\n",
    "{1+\\exp\\left(x_0 + \\sum_{j = 1}^d x_j s_{ij}\\right)}\\right)^{t_i}\n",
    "\\left(1-\\frac{\\exp\\left(x_0 + \\sum_{j = 1}^d x_j s_{ij} \\right)}{1+\\exp\\left(x_0 + \\sum_{j = 1}^d x_j s_{ij}\\right)}\\right)^{1-t_i} \\\\  %equations need punctuation\n",
    "& = \\prod_{i = 1}^m \n",
    "\\left(\\frac{\\exp\\left(x_0 + \\sum_{j = 1}^d x_j s_{ij} \\right)}\n",
    "{1+\\exp\\left(x_0 + \\sum_{j = 1}^d x_j s_{ij}\\right)}\\right)^{t_i}\n",
    "\\left(\\frac{1}{1+\\exp\\left(x_0 + \\sum_{j = 1}^d x_j s_{ij}\\right)}\\right)^{1-t_i} \\\\\n",
    "& = \\prod_{i = 1}^m \n",
    "\\left(\\frac{\\left[\\exp\\left(x_0 + \\sum_{j = 1}^d x_j s_{ij} \\right) \\right]^{t_i} }\n",
    "{1+\\exp\\left(x_0 + \\sum_{j = 1}^d x_j s_{ij}\\right)}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Function $f$ is defined under two parameters. Let a $m \\times d$ matrix $\\boldsymbol{S}$ and vector $\\boldsymbol{t}$ be given. Also, let $\\boldsymbol{S} = (s_{ij})_{i, j = 1}^{m,d}$ and $\\boldsymbol{t} = (t_1, t_2, t_3, ..., t_m)^T \\in \\{0,1\\}^m$. Matrix $\\boldsymbol{S}$ contains the values of the predictors, and vector $\\boldsymbol{x}$ stores values of binary responses.\n",
    "\n",
    "The purpose of using logistic regression is to find a way to predict an event in a binary setting. For example, what are the chances a coin will land tails, what are the chances this person is infected with covid-19,  or what are the chances it will precipitate that day. In this case, we want to find logistic regression model, so that we could determine the chances of getting admitted using student's ranking, gre score, and gpa.\n",
    "\n",
    "Below we have both $\\boldsymbol{S}$ and $\\boldsymbol{t}$. You can change \"n\" to be any integer between 0 and 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "data = genfromtxt('binary.csv', dtype=float, delimiter=',', skip_header = True)\n",
    "s = data[:n, 1:]\n",
    "t = data[:n, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our $\\boldsymbol{S}$ and $\\boldsymbol{t}$, now what we want to do is define our dimensions. For this to work we must have the number of rows in S + 1 for the dimensions to work according to function $f$. We will add 1 dimension to get the wanted results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no,dim_s = s.shape\n",
    "r = dim_s +1\n",
    "dim = r+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have our dimensions such that dim = d, so that $\\boldsymbol{x} = (x_0, x_1, x_2, ..., x_d)$. What we want is to estimate the integration of $f$ with respect to a normal prior. Which is,\n",
    "\n",
    "\\begin{align*}\n",
    "\\int_{\\mathbb{R}^{d+1}} f(\\boldsymbol{x}) \\pi(\\boldsymbol{x})\\, d \\boldsymbol{x}\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi(\\boldsymbol{x}) = \\frac{\\exp(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\nu})^T \\boldsymbol{\\mathsf{\\Sigma}}^{-1}(\\boldsymbol{x} - \\boldsymbol{\\nu}))}{\\sqrt{(2 \\pi)^{d+1}|\\mathbf{\\Sigma}|}}\n",
    "\\end{align*}\n",
    "\n",
    "such that the prior mean is $\\boldsymbol{\\nu}$ and prior covariance is $\\mathsf{\\Sigma}$. \n",
    "\n",
    "So that we can compute an array of $f(\\boldsymbol{x}_1), f(\\boldsymbol{x}_2), f(\\boldsymbol{x}_3), ... f(\\boldsymbol{x}_n)$ while $\\boldsymbol{x}_i \\sim \\pi$, we need to calculate,\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\boldsymbol{x}_k) = \\prod_{i = 1}^m \n",
    "\\left(\\frac{\\left[\\exp\\left(x_{k0} + \\sum_{j = 1}^d x_{kj} s_{ij} \\right)\\right] ^{t_i} }\n",
    "{1+\\exp\\left(x_{k0} + \\sum_{j = 1}^d x_{kj} s_{ij}\\right)} \\right), k = 1, 2, 3, ..., n.\n",
    "\\end{align*}.\n",
    "\n",
    "Now, let $r+1 \\in \\mathbb{N}$ such that $0 \\leq r \\leq d + 1$ and is the number of integrations we want to find,\n",
    "\n",
    "\\begin{align*}\n",
    "\\int_{\\mathbb{R}^{d+1}} f(\\boldsymbol{x}) \\pi(\\boldsymbol{x})\\, d \\boldsymbol{x}, \\int_{\\mathbb{R}^{d+1}} x_1 f(\\boldsymbol{x}) \\pi(\\boldsymbol{x})\\, d \\boldsymbol{x}, \\int_{\\mathbb{R}^{d+1}} x_2 f(\\boldsymbol{x}) \\pi(\\boldsymbol{x})\\, d \\boldsymbol{x}, \\int_{\\mathbb{R}^{d+1}} x_3 f(\\boldsymbol{x}) \\pi(\\boldsymbol{x})\\, d \\boldsymbol{x}, ..., \\int_{\\mathbb{R}^{d+1}} x_{r-1}f(\\boldsymbol{x}) \\pi(\\boldsymbol{x})\\, d \\boldsymbol{x}.\n",
    "\\end{align*}\n",
    "\n",
    "We want to calculate these integrations because they are related to the posterior means. In Bayesian inference, the posterior mean of $x_i$ while $0 \\leq i \\leq d$ is\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\int_{\\mathbb{R}^{d+1}} x_i  f(\\boldsymbol{x}) \\pi(\\boldsymbol{x})\\, d \\boldsymbol{x}}{\\int_{\\mathbb{R}^{d+1}}  f(\\boldsymbol{x}) \\pi(\\boldsymbol{x})\\, d \\boldsymbol{x}}\n",
    "\\end{align*}.\n",
    "\n",
    "The first integral, $\\int_{\\mathbb{R}^{d+1}} f(\\boldsymbol{x}) \\pi(\\boldsymbol{x})\\, d \\boldsymbol{x}$, is the denominator, while the other integrals are numerators. Now we calculate the integrations, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16430858  0.00694682 -0.65995462 -0.4110202 ]\n",
      "MeanVarDataRep (AccumulateData Object)\n",
      "    solution        [-0.164  0.007 -0.66  -0.411]\n",
      "    indv_error_bound [5.689e-06 5.075e-06 1.399e-07 2.111e-05 1.052e-05]\n",
      "    ci_low          [ 1.883e-04 -3.829e-05  1.248e-06 -1.547e-04 -9.313e-05]\n",
      "    ci_high         [ 1.997e-04 -2.814e-05  1.528e-06 -1.125e-04 -7.209e-05]\n",
      "    ci_comb_low     [-0.203  0.006 -0.821 -0.494]\n",
      "    ci_comb_high    [-0.141  0.008 -0.563 -0.361]\n",
      "    solution_comb   [-0.164  0.007 -0.66  -0.411]\n",
      "    flags_comb      [False False False False]\n",
      "    flags_indv      [False False False False False]\n",
      "    n_total         2^(16)\n",
      "    n               [4096. 4096.  512.  512.  512.]\n",
      "    replications    2^(4)\n",
      "    time_integrate  0.597\n",
      "CubQMCCLT (StoppingCriterion Object)\n",
      "    inflate         1.200\n",
      "    alpha           0.010\n",
      "    abs_tol         0\n",
      "    rel_tol         2^(-2)\n",
      "    n_init          2^(8)\n",
      "    n_max           2^(30)\n",
      "LR (Integrand Object)\n",
      "Gaussian (TrueMeasure Object)\n",
      "    mean            0\n",
      "    covariance      [1.e+00 1.e-04 1.e+00 1.e+00]\n",
      "    decomp_type     pca\n",
      "Sobol (DiscreteDistribution Object)\n",
      "    d               2^(2)\n",
      "    randomize       1\n",
      "    graycode        0\n",
      "    seed            2^(3)\n",
      "    mimics          StdUniform\n",
      "    dim0            0\n"
     ]
    }
   ],
   "source": [
    "lr = LR(Sobol(dim_s+1,seed=8), s_matrix = s, t = t, r = r, prior_variance=[1,1e-4,1,1])\n",
    "if r==0: raise Exception('require r>0')\n",
    "qmcclt = CubQMCCLT(lr,\n",
    "    abs_tol = 0,\n",
    "    rel_tol = .25,\n",
    "    n_init = 256,\n",
    "    n_max = 2 ** 30,\n",
    "    inflate = 1.2,\n",
    "    alpha = 0.01,\n",
    "    replications = 16,\n",
    "    error_fun = lambda sv,abs_tol,rel_tol: maximum(abs_tol,abs(sv)*rel_tol),\n",
    "    bound_fun = lambda phvl, phvh: (\n",
    "        minimum.reduce([phvl[1:dim]/phvl[0],phvl[1:dim]/phvh[0],phvh[1:dim]/phvl[0],phvh[1:dim]/phvh[0]]),\n",
    "        maximum.reduce([phvl[1:dim]/phvl[0],phvl[1:dim]/phvh[0],phvh[1:dim]/phvl[0],phvh[1:dim]/phvh[0]]),\n",
    "        sign(phvl[0])!=sign(phvh[0])),\n",
    "    dependency = lambda flags_comb: hstack((flags_comb.any(),flags_comb)))\n",
    "s,data = qmcclt.integrate()\n",
    "print(s)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our values for our integrations are outputted in our solution. These values are actually our coefficients. So, let two vectors $\\boldsymbol{c}$ and $\\boldsymbol{x}$, such that $\\boldsymbol{c} = (c_0, c_1, c_2, c_3, ..., c_{r-1})\\in \\mathbb{R}^r$ and $\\boldsymbol{x} = (x_1, x_2, x_3, ..., x_{r-1}) \\in \\mathbb{R}^{r-1}$. Let $g: \\mathbb{R}^{r-1} \\to (0, 1)$ be a function. To map our function $g$ we have,\n",
    "\n",
    "\\begin{align*}\n",
    "g(\\boldsymbol{x}) = \\frac{\\exp(c_0 + c_1 x_1 + c_2 x_2 + c_3 x_3 + ... + c_{r-1} x_{r-1})}{1 + \\exp(c_0 + c_1 x_1 + c_2 x_2 + c_3 x_3 + ... + c_{r-1} x_{r-1})}\n",
    "\\end{align*}\n",
    "\n",
    "So, we have our logistic regression function to help predict things in a binary setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we want to ask is how accurate these functions are. Like in linear regression there is a way to use error bound to see how accurate the function is. We have our lower and higher bounds outputted in the data so like we did for function $g$ we have, \"ci_comb_low\" which is the lower bound and \"ci_comb_high\" as our higher bound. For this example let $g_l: \\mathbb{R}^{r-1} \\to (0,1)$ be the lower bound function and $g_h: \\mathbb{R}^{r-1} \\to (0,1)$ be the higher bound function. Let two vectors exist such that $\\boldsymbol{l} = (l_0, l_1, l_2, l_3, ..., l_{r-1})\\in \\mathbb{R}^r$ and $\\boldsymbol{h} = (h_0, h_1, h_2, h_3, ..., h_{r-1})\\in \\mathbb{R}^r$. To map functions $g_l$ and $g_h$ we have,\n",
    "\n",
    "\\begin{align*}\n",
    "g_l(\\boldsymbol{x}) = \\frac{\\exp(l_0 + l_1 x_1 + l_2 x_2 + l_3 x_3 + ... + l_{r-1} x_{r-1})}{1 + \\exp(l_0 + l_1 x_1 + l_2 x_2 + l_3 x_3 + ... + l_{r-1} x_{r-1})}\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "g_h(\\boldsymbol{x}) = \\frac{\\exp(h_0 + h_1 x_1 + h_2 x_2 + h_3 x_3 + ... + h_{r-1} x_{r-1})}{1 + \\exp(h_0 + h_1 x_1 + h_2 x_2 + h_3 x_3 + ... + h_{r-1} x_{r-1})}\n",
    "\\end{align*}.\n",
    "\n",
    "Thus, we have our error bounds for our logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cbc6a2e523825879ba6928de6d48dd415d053b3b6e498effe5c784f561b4217"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
